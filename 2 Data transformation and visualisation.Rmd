---
title: "Data modelling and model evaulation"
output: html_notebook
---
```{r, message=FALSE, warning=FALSE}
library(reshape2)
library(e1071)
library(stringr)
library(caret)
library(corrplot)
library(GGally)
library(ggfortify)
library(mlbench)
library(gbm)
library(parallel)
library(doParallel)
library(Rtsne)
library(tidyverse)
```
# Data Tranformation
## Data Tranformation for individual predictors
While in the previous part we were focused on bringing the data from a point 
representation into a trip representation here we are concerned with 
transforming the data into a state that makes it suitable for modelling. For 
instance by scaling and centering the data.
```{r}
# Import data if not already loaded.
if(!exists("trips_stats_fl")){
 trips_stats_fl <- read_tsv("./dataset/out/trips_stats_fl.tsv")
}


tmp_dropcols <- c("VESSEL_TYPE", "TRIP_ID", "MMSI", "time_start", "time_end")
tmp_areacols <- names(trips_stats_fl) %>% str_extract("area_\\d*") %>% na.omit() %>% as.vector()

# Create x and y data. 
xdata <- trips_stats_fl %>%
  ungroup() %>% 
  select(-one_of(tmp_dropcols))
# ydata <- trips_stats_fl$VESSEL_TYPE %>% as.factor()

ydata <- plyr::mapvalues(trips_stats_fl$VESSEL_TYPE,
  from = c("Bulk Carrier", "Container Ship", "Fishing Vessel", 
           "High Speed Craft", "Passenger", "Pleasure Craft", "Port Tender",
           "Tanker"), to = 1:8) %>% as.factor()



# Apply log+1 transformation and scaling. 
xdata_transformed <- sapply(xdata, function(x) scale(log(1+x))) %>% data.frame()

yxdata_transformed <- cbind(ydata, xdata_transformed)
yxdata <- cbind(ydata, xdata)

write_tsv(yxdata_transformed, "dataset/out/yxdata_transformed.tsv")
write_tsv(yxdata, "dataset/out/yxdata.tsv")
```

Below we plot density plots for the semantic features and the trip statistics we
have previously created. We observe that for the semantic features the 
distributions appear to be strongly skewed. This is partly due to the fact that 
most vessel will have spent some time only in couple of the dock areas of the 
port we have defined. Cosnequently most values will be 0.  Moreover, as noted by
/cite{Kuhn} an apparent skew may be exaberated by the relatively small sample 
size. For instance variable area_17 which appears to be extremely skewed had 
only 7 non-zero measurements. 

Next we plot the densities of the features describing the various
vesselstatistics we have previously calculated. The densities of the remaining
feautures also show considerable skewness in particular the
"duration_total_minutes" shows some strong right-skew.

Skewed input data can prevent a machine learning model to to porperly use the
information. For instance while decision trees are unaffected by skewed data
neural networks, which use the raw values for the calculations are. A common
solution is the binning of continuous values but this is inevitably lead to a
loss of information. A better approach is to apply a log-transformation to the
data. /cite{Berry Data mining techniques}

Finally, the features have very different measurement scales which can lead to
severe problems. As a standard procedure we normalise each feature to have 
a 0 mean and standard deviation of 1. This also helps with the training of a
neural netwwork classifier because convergence is usually faster when using
normalized inputs /cite{Lecun efficient backprop}.
```{r, echo=FALSE, message=FALSE, warning=FALSE}
# Plot area densities
plot_area_densities <- xdata[, tmp_areacols] %>% 
  melt() %>% 
  ggplot() +
  geom_density(mapping = aes(x=value/60/60), color="black", fill="blue", alpha=0.3) +
  scale_x_continuous(breaks=c(0, 1, 2, 4, 8, 16, 32, 64),trans="log1p", expand=c(0,0)) +
  scale_y_continuous(breaks=c(0, 0.25, 0.5, 1, 2, 4),trans="log1p", expand=c(0,0)) +
  labs(title="Densityes of area features", x="Time spent in area [hours]", y="Density") +
  facet_wrap(~ variable, ncol = 5, scales = "free")

xdata[, tmp_areacols] %>% 
  melt() %>% group_by(variable) %>% filter(value > 0) %>% count() %>% arrange(n)

plot_vars_histo <- xdata[,0:6]%>%
  melt() %>%
  ggplot() +
  geom_histogram(mapping = aes(x=value), color="black", fill="blue", alpha=0.3) +
  theme(text = element_text(size=8)) +
  facet_wrap(~ variable, ncol = 3, scales = "free")
plot_vars_histo
```

Below we have plotted the log-transformed and normalised input data. In other 
words we applied a log transformation, we centered the data by subtracting the 
mean and standardized the data by dividing every value through their standard 
deviation. #TODO Combine before after of certain variables. 

Plot the stuff...

```{r, message=FALSE, warning=FALSE}
plot_scalelogvars_densities <- xdata_transformed[,c(0:6)] %>%
  melt() %>%
  ggplot() +
  geom_histogram(mapping = aes(x=value), color="black", fill="blue", alpha=0.3) +
  theme(text = element_text(size=8)) +
  facet_wrap(~ variable, ncol = 3, scales = "free")
plot_scalelogvars_densities
```

# Investigate relationship between features
The linear correlation betweeen the feautures is higher for the transformed
dataset compared to the original data. Thid suggests that there is a nonlinear
relationship between the features. For instance the distance covered within each
segment correlates quite strongly with the total distance covered after each
trip. When using the spearman correlation, which is invariant to transformaiton
the measured correlation before and after the data transformation remains 
unchanged. 
```{r}
segCorr <- cor(xdata_transformed, method = "pearson")
corrplot(segCorr, order = "original", tl.cex = .7, tl.col = "black")

segCorr2 <- cor(xdata, method = "pearson")
corrplot(segCorr2, order = "original", tl.cex = .7, tl.col = "black")
```
Below we plot again the correlation matrix for just the features that showed
some correltion before. We can see how the data-transformation did increase the 
pearson correlation between the features. We will consider removing some feature
since they appear to express the same information anyway. For instance the
distance covered within a segment is a function of the overall average speed of
the vessel and the duration of the segment.
```{r, message=FALSE, warning=FALSE}
plot_corrxtrans <- cbind(ydata, xdata_transformed) %>% ggscatmat(columns = 1:6, color="ydata", alpha=0.1, corMethod="pearson")
ggsave("figures/plot_corrxtrans.png", plot_corrxtrans)
plot_corrxtrans
```

## PCA
We carry out a principal component analysis projecting the data down to 2
dimensions. We find that the data does not seem to be easily separable across
the classes. However since the invention of PCA (1930s) more sophisticated
techniques have been developed which are able to take into account non-linear
relations between the data. Next we project the data onto 2 dimensions using
T-SNE developed by Laurens van der Maaten.
```{r}
pca_object <- prcomp(yxdata_transformed[,2:21], center = TRUE, scale = TRUE)
percentVariance <- pca_object$sd^2/sum(pca_object$sd^2)*100
# percentVariance
# head(pca_object$rotation[, 1:3])

autoplot(pca_object, data = yxdata_transformed, colour = 'ydata', alpha=1, size=3)
```

## T-Sne
T-Sne is a non-linear algorithm that projects a higher dimensional input to a 
lower dimensional representation such as a 2D plane. The algorithm also adapts 
to the underlying data and performs different transformations on different 
regions. Crucially, it is necessary to try different hyper-parameter settings
(the main one being the perplexity score) to get a meaningful
representation\cite{wattenberg2016how}.

Below we plotted the final result after trying different hyper parameter
settings. We can observe that t-sne was able to separate some vessel types quite
nicely while others do not seem to be easily separable. 

We can also see that there are also multiple clusters for the same
class. For instance for the pleasure crafts. This could be due to... TODO.
```{r, eval=FALSE, warning=TRUE, include=FALSE}
# Parameter settings
perplexity_list <- c(120, 140, 160, 180, 200, 220, 240)
perplexity_list <- c(120)  # Only use 120 for now
maxiter <- 5000

for (i in perplexity_list) {
  set.seed(1234)
  tsne <- Rtsne(yxdata_transformed[,2:21], dims = 2, perplexity = i, 
                verbose=FALSE, max_iter = maxiter)
  
  embedding <- as.data.frame(tsne$Y)
  embedding$Class <- as.factor(yxdata_transformed$ydata)
  
  plot_tsne <- ggplot(embedding, aes(x = V1, y = V2, color = Class)) +
    geom_point(size = 2, alpha = 0.2)
  ggsave(filename = paste("./figures/tsne_perp_", i,"_",maxiter,".png"), plot = plot_tsne)
}
plot_tsne
```