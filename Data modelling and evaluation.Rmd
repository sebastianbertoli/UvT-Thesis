---
title: "Data modelling and model evaulation"
output: html_notebook
---
```{r, message=FALSE, warning=FALSE}
library(tidyverse)
library (reshape2)
library(e1071)
library(stringr)
library(caret)
library(corrplot)
library(GGally)
library(ggfortify)
library(mlbench)
library(gbm)
library(parallel)
library(doParallel)
library(Rtsne)
```

```{r, message=FALSE, warning=FALSE}
# Import data if not already loaded.
if(!exists("trips_stats_fl")){
 trips_stats_fl <- read_tsv("./dataset/out/trips_stats_fl.tsv")
}
```
# Data Tranformation
## Data Tranformation for individual predictors
While in the previous part we were focused on bringing the data from a point 
representation into a trip representation here we are concerned with 
transforming the data into a state that makes it suitable for modelling. For 
instance by scaling and centering the data.
```{r}
tmp_dropcols <- c("VESSEL_TYPE", "TRIP_ID", "MMSI", "time_start", "time_end")
tmp_areacols <- names(trips_stats_fl) %>% str_extract("area_\\d*") %>% na.omit() %>% as.vector()

# Create x and y data. 
xdata <- trips_stats_fl %>%
  ungroup() %>% 
  select(-one_of(tmp_dropcols))
ydata <- trips_stats_fl$VESSEL_TYPE %>% as.factor()

# Apply log+1 transformation and scaling. 
xdata_transformed <- sapply(xdata, function(x) scale(log(1+x))) %>% data.frame()

yxdata_transformed <- cbind(ydata, xdata_transformed)
yxdata <- cbind(ydata, xdata)
```

Below we plot density plots for the semantic features and the trip statistics we
have previously created. We observe that for the semantic features the 
distributions appear to be strongly skewed. This is partly due to the fact that 
most vessel will have spent some time only in couple of the dock areas of the 
port we have defined. Cosnequently most values will be 0.  Moreover, as noted by
/cite{Kuhn} an apparent skew may be exaberated by the relatively small sample 
size. For instance variable area_17 which appears to be extremely skewed had 
only 7 non-zero measurements. 

Next we plot the densities of the features describing the various
vesselstatistics we have previously calculated. The densities of the remaining
feautures also show considerable skewness in particular the
"duration_total_minutes" shows some strong right-skew.

Skewed input data can prevent a machine learning model to to porperly use the
information. For instance while decision trees are unaffected by skewed data
neural networks, which use the raw values for the calculations are. A common
solution is the binning of continuous values but this is inevitably lead to a
loss of information. A better approach is to apply a log-transformation to the
data. /cite{Berry Data mining techniques}

Finally, the features have very different measurement scales which can lead to
severe problems. As a standard procedure we normalise each feature to have 
a 0 mean and standard deviation of 1. This also helps with the training of a
neural netwwork classifier because convergence is usually faster when using
normalized inputs /cite{Lecun efficient backprop}.
```{r, echo=FALSE, message=FALSE, warning=FALSE}
# Plot area densities
plot_area_densities <- xdata[, tmp_areacols] %>% 
  melt() %>% 
  ggplot() +
  geom_density(mapping = aes(x=value/60/60), color="black", fill="blue", alpha=0.3) +
  scale_x_continuous(breaks=c(0, 1, 2, 4, 8, 16, 32, 64),trans="log1p", expand=c(0,0)) +
  scale_y_continuous(breaks=c(0, 0.25, 0.5, 1, 2, 4),trans="log1p", expand=c(0,0)) +
  labs(title="Densityes of area features", x="Time spent in area [hours]", y="Density") +
  facet_wrap(~ variable, ncol = 5, scales = "free")

xdata[, tmp_areacols] %>% 
  melt() %>% group_by(variable) %>% filter(value > 0) %>% count() %>% arrange(n)

plot_vars_histo <- xdata[,0:6]%>%
  melt() %>%
  ggplot() +
  geom_histogram(mapping = aes(x=value), color="black", fill="blue", alpha=0.3) +
  theme(text = element_text(size=8)) +
  facet_wrap(~ variable, ncol = 3, scales = "free")
plot_vars_histo
```

Below we have plotted the log-transformed and normalised input data. In other 
words we applied a log transformation, we centered the data by subtracting the 
mean and standardized the data by dividing every value through their standard 
deviation. #TODO Combine before after of certain variables. 

Plot the stuff...

```{r, message=FALSE, warning=FALSE}
plot_scalelogvars_densities <- xdata_transformed[,c(0:6)] %>%
  melt() %>%
  ggplot() +
  geom_histogram(mapping = aes(x=value), color="black", fill="blue", alpha=0.3) +
  theme(text = element_text(size=8)) +
  facet_wrap(~ variable, ncol = 3, scales = "free")
plot_scalelogvars_densities
```

# Investigate relationship between features
The linear correlation betweeen the feautures is higher for the transformed
dataset compared to the original data. Thid suggests that there is a nonlinear
relationship between the features. For instance the distance covered within each
segment correlates quite strongly with the total distance covered after each
trip. When using the spearman correlation, which is invariant to transformaiton
the measured correlation before and after the data transformation remains 
unchanged. 
```{r}
segCorr <- cor(xdata_transformed, method = "pearson")
corrplot(segCorr, order = "original", tl.cex = .7, tl.col = "black")

segCorr2 <- cor(xdata, method = "pearson")
corrplot(segCorr2, order = "original", tl.cex = .7, tl.col = "black")
```
Below we plot again the correlation matrix for just the features that showed
some correltion before. We can see how the data-transformation did increase the 
pearson correlation between the features. We will consider removing some feature
since they appear to express the same information anyway. For instance the
distance covered within a segment is a function of the overall average speed of
the vessel and the duration of the segment.
```{r, message=FALSE, warning=FALSE}
plot_corrxtrans <- cbind(ydata, xdata_transformed) %>% ggscatmat(columns = 1:6, color="ydata", alpha=0.1, corMethod="pearson")
ggsave("figures/plot_corrxtrans.png", plot_corrxtrans)
plot_corrxtrans
```

## PCA
Data does not appear to be easily linearly separable. 
```{r}
pca_object <- prcomp(yxdata_transformed[,2:21], center = TRUE, scale = TRUE)
# percentVariance <- pca_object$sd^2/sum(pca_object$sd^2)*100
# percentVariance
# head(pca_object$rotation[, 1:3])

autoplot(pca_object, data = yxdata_transformed, colour = 'ydata', alpha=1, size=3)
```

## T-Sne
blablabla
```{r}

perplexity_list <- c(80, 120)

for (i in perplexity_list){
  set.seed(1234)
  tsne <- Rtsne(yxdata_transformed[,2:21], dims = 2, perplexity=i, 
                verbose=FALSE, max_iter = 10000)
  
  embedding <- as.data.frame(tsne$Y)
  embedding$Class <- as.factor(yxdata_transformed$ydata)
  
  plot_tsne <- ggplot(embedding, aes(x=V1, y=V2, color=Class)) +
    geom_point(size=2, alpha=0.2)
  ggsave(filename = paste("./figures/tsne_perp_", i, "_10000.png"), plot = plot_tsne)
}


```


# Machine Learning Experiments
```{r}

yxdata_transformed$ydata <- mapvalues(
  yxdata_transformed$ydata,
  from = c("Bulk Carrier", "Container Ship", "Fishing Vessel", 
           "High Speed Craft", "Passenger", "Pleasure Craft", "Port Tender",
           "Tanker"), to = 1:8) %>% as.factor()
head(yxdata_transformed)
```


```{r}
# inTraining <- createDataPartition(Sonar$Class, p = .75, list = FALSE)
# training <- Sonar[ inTraining,]
# testing  <- Sonar[-inTraining,]

fitControl <- trainControl(## 10-fold CV
                           method = "repeatedcv",
                           number = 10,
                           ## repeated ten times
                           repeats = 10, 
                           allowParallel = FALSE, 
                           sampling = "down")

set.seed(825)
gbmFit1 <- train(ydata ~ ., data = yxdata_transformed[1:6], 
                 method = "gbm", 
                 trControl = fitControl,
                 ## This last option is actually one
                 ## for gbm() that passes through
                 verbose = FALSE)

ggplot(gbmFit1)
```
